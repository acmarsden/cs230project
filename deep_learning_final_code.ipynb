{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "#import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = np.loadtxt('X_train.csv', delimiter=',')\n",
    "X_dev = np.loadtxt('X_dev.csv',delimiter=',')\n",
    "X_test = np.loadtxt('X_test.csv',delimiter=',')\n",
    "Y_train = np.loadtxt('Y_train.csv', delimiter=',')\n",
    "Y_dev = np.loadtxt('Y_dev.csv', delimiter=',')\n",
    "Y_test = np.loadtxt('Y_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.reshape(Y_train, (1, 5000))\n",
    "Y_dev = np.reshape(Y_dev, (1, 1070))\n",
    "Y_test = np.reshape(Y_test, (1, 1067))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 5000\n",
      "number of dev examples = 1070\n",
      "number of test examples = 1067\n",
      "X_train shape: (939, 5000)\n",
      "Y_train shape: (1, 5000)\n",
      "X_dev shape: (939, 1070)\n",
      "Y_dev shape: (1, 1070)\n",
      "X_test shape: (939, 1067)\n",
      "Y_test shape: (1, 1067)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print (\"number of dev examples = \" + str(X_dev.shape[1]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(Y_dev.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's just try with activate/no activate\n",
    "Y_train = 1.0*(Y_train!=0)\n",
    "Y_dev = 1.0*(Y_dev!=0)\n",
    "Y_test = 1.0*(Y_test!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of a small molecule vector (939)\n",
    "    n_y -- scalar = 1\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    " \n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape = [n_x, None], name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y, None], name = \"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(dims):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        Wk : [dims[k],dims[k-1]]\n",
    "                        bk : [dims[k],1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, ..., Wn, bn\n",
    "    \"\"\"\n",
    "    tf.set_random_seed(1)  \n",
    "    n = np.size(dims)\n",
    "\n",
    "    assert dims[0]==939\n",
    "    assert dims[n-1] == 1\n",
    "    parameters = {}\n",
    "    \n",
    "    for kk in range(n-1):\n",
    "        W = tf.get_variable(\"W\"+str(kk+1), [dims[kk+1],dims[kk]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        b = tf.get_variable(\"b\"+str(kk+1), [dims[kk+1],1], initializer = tf.zeros_initializer())\n",
    "        parameters[\"W\"+str(kk+1)] = W\n",
    "        parameters[\"b\"+str(kk+1)] = b\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z_final -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(parameters)/2\n",
    "    \n",
    "    if n>1:\n",
    "        for kk in range(n-1):\n",
    "            W_kk = parameters['W'+str(kk+1)]\n",
    "            b_kk = parameters['b'+str(kk+1)]\n",
    "            if kk == 0:\n",
    "                Z = tf.add(tf.matmul(W_kk,X),b_kk)\n",
    "            else:\n",
    "                Z = tf.add(tf.matmul(W_kk,A),b_kk)\n",
    "            A = tf.nn.relu(Z) \n",
    "        W_final = parameters['W'+str(n)]\n",
    "        b_final = parameters['b'+str(n)]\n",
    "        Z_final = tf.sigmoid(tf.add(tf.matmul(W_final,A),b_final))                               \n",
    "    \n",
    "    else:\n",
    "        W_final = parameters['W'+str(n)]\n",
    "        b_final = parameters['b'+str(n)]\n",
    "        Z_final = tf.sigmoid(tf.add(tf.matmul(W_final,X),b_final))      \n",
    "        \n",
    "    return Z_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- output of forward propagation (output of the last LINEAR unit), of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    eps = 10e-3\n",
    "    logits = tf.transpose(Z) \n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    #cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, logits = logits))\n",
    "    #cost = tf.add(tf.reduce_mean(tf.nn.l2_loss(Z3-Y)), tf.reduce_mean(tf.nn.l2_loss(tf.multiply(Y,Z3-Y))))\n",
    "    cost = -tf.reduce_mean(tf.add(scale*tf.multiply(labels, tf.log(logits+eps)), tf.multiply(1-labels, tf.log(1-logits+eps))))\n",
    "    #cost = tf.reduce_mean(tf.multiply(labels, tf.log(logits+eps)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, dims, learning_rate = 0.01,\n",
    "          num_epochs = 1000, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(dims)\n",
    "    \n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z_final = forward_propagation(X, parameters)\n",
    "    \n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z_final, Y)\n",
    "    \n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            _ , epoch_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        #plt.plot(np.squeeze(costs))\n",
    "        #plt.ylabel('cost')\n",
    "        #plt.xlabel('iterations (per tens)')\n",
    "        #plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        #plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "        \n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z_final), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X_2:0\", shape=(939, ?), dtype=float32)\n",
      "Y = Tensor(\"Y_2:0\", shape=(1, ?), dtype=float32)\n",
      "Cost after epoch 0: 7.251016\n",
      "Cost after epoch 100: 1.471244\n",
      "Cost after epoch 200: 1.194067\n",
      "Cost after epoch 300: 1.038743\n",
      "Cost after epoch 400: 0.929934\n",
      "Cost after epoch 500: 0.835980\n",
      "Cost after epoch 600: 0.756696\n",
      "Cost after epoch 700: 0.693663\n",
      "Cost after epoch 800: 0.630854\n",
      "Cost after epoch 900: 0.575749\n",
      "Cost after epoch 1000: 0.527550\n",
      "Cost after epoch 1100: 0.489406\n",
      "Cost after epoch 1200: 0.459536\n",
      "Cost after epoch 1300: 0.429621\n",
      "Cost after epoch 1400: 0.405958\n",
      "Cost after epoch 1500: 0.385143\n",
      "Cost after epoch 1600: 0.367081\n",
      "Cost after epoch 1700: 0.351182\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HYPERPARAMETER SEARCH\n",
    "The hyperparameters we will explore in this model are:\n",
    "    -- Number of layers (1-5)\n",
    "    -- dimension of each layer\n",
    "    -- scale value (upweights the cost function on the positives)\n",
    "\"\"\"\n",
    "Hyperparam_data = {}\n",
    "\n",
    "# #Layers=1 (i.e. linear regression)\n",
    "\n",
    "dims = [939, 1]\n",
    "#scale_values = np.random.randint(1,9, size = 10)*np.power(10.0, np.random.randint(-1,3, size = 10))\n",
    "scale_values = [90]\n",
    "for ii in range(np.size(scale_values)):\n",
    "    X, Y = create_placeholders(939, 1)\n",
    "    print (\"X = \" + str(X))\n",
    "    print (\"Y = \" + str(Y))\n",
    "    scale = scale_values[ii]\n",
    "    parameters = model(X_train, Y_train, X_test, Y_test, dims, 1, 5000)\n",
    "    with tf.Session() as sess:\n",
    "        X, Y = create_placeholders(939,1)\n",
    "        Z_train = sess.run(forward_propagation(X, parameters), feed_dict={X: X_train})\n",
    "        Z_dev = sess.run(forward_propagation(X, parameters), feed_dict={X: X_dev})\n",
    "    P_train = Z_train >= 0.5\n",
    "    P_dev = Z_dev>=0.5\n",
    "    true_positive = (Y_dev*P_dev).sum()\n",
    "    false_negative = (Y_dev*(1-P_dev)).sum()\n",
    "    false_positive = ((1-Y_dev)*P_dev).sum()\n",
    "    false_positive\n",
    "    F1 = (2*true_positive)/(2*true_positive + false_negative + false_positive)\n",
    "    print(\"F1 Score:\", F1)\n",
    "    Hyperparam_data['Test'+str(ii)] = {'scale': scale, 'n_layers': 1, 'F1': F1}\n",
    "    \n",
    " \n",
    "\n",
    "dim_values = [360]\n",
    "scale_values = [120]\n",
    "for ii in range(np.size(scale_values)):\n",
    "    dims = [939, dim_values[ii],1]\n",
    "    X, Y = create_placeholders(939, 1)\n",
    "    print (\"X = \" + str(X))\n",
    "    print (\"Y = \" + str(Y))\n",
    "    scale = scale_values[ii]\n",
    "    parameters = model(X_train, Y_train, X_test, Y_test, dims, 0.01, 2000)\n",
    "    with tf.Session() as sess:\n",
    "        X, Y = create_placeholders(939,1)\n",
    "        Z_train = sess.run(forward_propagation(X, parameters), feed_dict={X: X_train})\n",
    "        Z_dev = sess.run(forward_propagation(X, parameters), feed_dict={X: X_dev})\n",
    "        Z_test = sess.run(forward_propagation(X, parameters), feed_dict={X: X_test})\n",
    "    P_train = Z_train >= 0.5\n",
    "    P_dev = Z_dev>=0.5\n",
    "    P_test = Z_test>0.5\n",
    "    true_positive = (Y_dev*P_dev).sum()\n",
    "    false_negative = (Y_dev*(1-P_dev)).sum()\n",
    "    false_positive = ((1-Y_dev)*P_dev).sum()\n",
    "    F1_dev = (2*true_positive)/(2*true_positive + false_negative + false_positive)\n",
    "    true_positive = (Y_test*P_test).sum()\n",
    "    false_negative = (Y_test*(1-P_test)).sum()\n",
    "    false_positive = ((1-Y_test)*P_test).sum()\n",
    "    F1_test = (2*true_positive)/(2*true_positive + false_negative + false_positive)\n",
    "    print(\"F1 Score Dev:\", F1_dev)\n",
    "    print(\"F1 Score Test:\", F1_test)\n",
    "    Hyperparam_data['NewTest'+2*str(ii)] = {'scale': scale, 'n_layers': 2, 'dim_value':dim_values[ii], 'F1_dev': F1_dev, 'F1_test': F1_test}\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "dim_values1 = [75, 450, 700]\n",
    "scale_values = [5, 5,5]\n",
    "for ii in range(np.size(scale_values)):\n",
    "    dim_value1 = dim_values1[ii]\n",
    "    dim_value2 = np.random.randint(1, dim_value1)\n",
    "    dims = [939, dim_value1, dim_value2, 1]\n",
    "    X, Y = create_placeholders(939, 1)\n",
    "    print (\"X = \" + str(X))\n",
    "    print (\"Y = \" + str(Y))\n",
    "    scale = scale_values[ii]\n",
    "    parameters = model(X_train, Y_train, X_test, Y_test, dims, 0.0001, 2000)\n",
    "    with tf.Session() as sess:\n",
    "        X, Y = create_placeholders(939,1)\n",
    "        Z_train = sess.run(forward_propagation(X, parameters), feed_dict={X: X_train})\n",
    "        Z_dev = sess.run(forward_propagation(X, parameters), feed_dict={X: X_dev})\n",
    "        Z_test = sess.run(forward_propagation(X, parameters), feed_dict={X: X_test})\n",
    "    P_train = Z_train >= 0.5\n",
    "    P_dev = Z_dev>=0.5\n",
    "    P_test = Z_test>0.5\n",
    "    true_positive = (Y_dev*P_dev).sum()\n",
    "    false_negative = (Y_dev*(1-P_dev)).sum()\n",
    "    false_positive = ((1-Y_dev)*P_dev).sum()\n",
    "    F1_dev = (2*true_positive)/(2*true_positive + false_negative + false_positive)\n",
    "    true_positive = (Y_test*P_test).sum()\n",
    "    false_negative = (Y_test*(1-P_test)).sum()\n",
    "    false_positive = ((1-Y_test)*P_test).sum()\n",
    "    F1_test = (2*true_positive)/(2*true_positive + false_negative + false_positive)\n",
    "    print(\"F1 Score Dev:\", F1_dev)\n",
    "    print(\"F1 Score Test:\", F1_test)\n",
    "\n",
    "    Hyperparam_data['Test'+5*str(ii)] = {'scale': scale, 'n_layers': 3, 'dim_value1':dim_value1, 'dim_value2':dim_value2, 'F1_dev': F1_dev, 'F1_test':F1_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.save('hyperparamdatscaling.npy', Hyperparam_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
